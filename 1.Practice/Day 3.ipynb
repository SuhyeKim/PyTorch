{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # building blocks\n",
    "import torch.nn.functional as F # convolution functions\n",
    "import torch.optim as optim # various optimization algorithms\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111fc75d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will use fake data for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "y_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 Pytorch는 NCHW 형태이다.<br>\n",
    "tensor는 메모리에 저장할 때 contiguous 형태로 저장되는데 <br>\n",
    "개수(N), 높이(H), 너비(W), 컬러(C)\n",
    "NCHW는 채널단위로 각 픽셀의 값이 연속적으로 저장되고 <br> \n",
    "NHWC는 픽셀 단위로 채널의 값이 연속적으로 저장된다. <br>\n",
    "\n",
    "예를 들어, 아래 그림에서 메모리 저장 방식은 다음과 같다. <br>\n",
    "![Day3](/Volumes/SUHYE/1.research/2024_study/PyTorch/1.Practice/image.png)\n",
    "\n",
    "- NCHW 방식의 메모리<br>\n",
    "[0.2 0.4 0.7 0.8 1.5 2.4 6.5 1.2 ... 0.8 0.9 1.4.6.5 ...]<br>\n",
    "- NHWC 방식의 메모리(tensor)<br>\n",
    "[0.2 0.8 0.4 0.9 0.7 1.4 0.8 6.5 ....]<br>\n",
    "\n",
    "tensor 연산은 효율적인 연산을 위해 병렬적으로 수행된다.<br>\n",
    "따라서 실제 convolution과 같은 연산을 수행할 때 연산에 필요한 값을 벡터화한 후 연산을 수행하게 된다.<br>\n",
    "\n",
    "#### 장점 <br>\n",
    "채널 단위로 벡터화하여 연산이 수행되기 때문에 NHWC로 메모리에 저장하는 것이 메모리 엑세스 관점에서 효율적이고, FP16일 때는 메모리 대역폭을 FP32보다 더욱 효율적으로 사용할수 있기 대문에 가속에 큰 장점을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "torch.Tenso() 클래스에서 [.requires_grad=True] 를 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(Track) 한다. <br>\n",
    "계산이 완료된 후 .backward()를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있다.\n",
    "이 Tensor의 변화도는 .grad 속성에 누적된다.\n",
    "\n",
    "- Tensor가 기록을 추적하는 것을 중단하려면 [.detach()]를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있다.\n",
    "- 기록을 추적하는 것과 메모리를 사용하는 것을 방지하기 위해, 코드 블럭을 with torch.no_grad()로 감쌀 수 있다.\n",
    "- 특히 변화도(gradient)는 필요 없지만, [.requires_grad=True]가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evalutate)할 때 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "print('W: ', W)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = Wx + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print('hypothesis: \\n', hypothesis)\n",
    "print('\\n')\n",
    "print('y_train: \\n', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print('cost: \\n', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "optimizer.zero_grad() # gradient 초기화\n",
    "cost.backward() # gradient 계산\n",
    "optimizer.step() # 개선된 gradient의 방향대로 W, b 개선\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('W:\\n',W)\n",
    "print('b: \\n',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the hypothesis is now better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print('hypothesis: \\n', hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Full Code\n",
    "In reality, we will be training on the dataset for multiple epochs. <br>\n",
    "This can be done simply with loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "y_train = torch.FloatTensor([[1],\n",
    "                             [2],\n",
    "                             [3]])\n",
    "\n",
    "# Hypothesis Initialization\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# Model Initialization\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "# Epoch\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) \n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2) # square\n",
    "\n",
    "    # Upgrade H(x) using cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print log\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
