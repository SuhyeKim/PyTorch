{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5 Logistic Regression\n",
    "\n",
    "logistic regression은 binary classification 문제이다.<br>\n",
    "예를 들어 d size의 벡터에 m개의 데이터가 들어있다고 한다면, 이 행렬을 m개의 0과 1로 이루어진 정답을 구하는 모델을 학습해야 할 것이다. 이것이 binary calssification의 세팅이 될 것이다.\n",
    "\n",
    "$$ X \\in \\mathbb{R}^{m * d} $$\n",
    "\n",
    "어떤 확률 P가 1일 확률은 1에서 어떤 확률 P가 0일 확률을 뺀 값일 것이다.\n",
    "\n",
    "$$ P(X = 1) = 1 - P(X = 0) $$\n",
    "\n",
    "Logistic Regression의 Weight parameter는 보통 W로 불리우는데,<br>\n",
    "W는 (d,1) 개의 weight parameter가 들어있다.\n",
    "\n",
    "$$ W \\in \\mathbb{R}^{d * 1} $$\n",
    "\n",
    "X와 W를 곱한 후, sigmoid 함수를 사용하여 0과 1에 근사하는 값을 도출한다.\n",
    "\n",
    "$$ Sigmoid 함수 = \n",
    "\n",
    "-\\infty 는 \n",
    "0,\n",
    "+\\infty 는 1 $$\n",
    "의 값을 갖게 해주는 함수이다:\n",
    "\n",
    "$$ \\sigma = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "이를 차용한 것이 H(X)이다.<br>\n",
    "H(X)는 P(X=1), 즉 X가 1일 확률을 의미한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x122b58790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2],\n",
    "          [2,3],\n",
    "          [3,1],\n",
    "          [4,3],\n",
    "          [5,3],\n",
    "          [6,2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "x_test = torch.FloatTensor(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has a `torch.exp()` function that resembles the exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals:  tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to compute the hypothesis function conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  tensor([[0.],\n",
      "        [0.]], requires_grad=True)\n",
      "b:  tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "print('W: ', W)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W)+b)))\n",
    "\n",
    "# 또는 이렇게도 표현할 수 있다.\n",
    "hypothesis2 = 1 / (1+ torch.exp(-torch.matmul(x_train,W)+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.matmul(W))\n",
    "\n",
    "print(hypothesis)\n",
    "print(hypothesis.shape)\n",
    "\n",
    "print(hypothesis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has a `torch.sigmoid()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/(1+e^{-1}):  tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print('1/(1+e^{-1}): ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(-(x_train.matmul(W)+b))\n",
    "\n",
    "\n",
    "print(hypothesis)\n",
    "print(hypothesis.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to measure the difference between `hypothesis` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis: \n",
      " tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      " y_train: \n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print('hypothesis: \\n',hypothesis)\n",
    "print('\\n y_train: \\n',y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Cost Function\n",
    "For the element, the loss can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0] * torch.log(hypothesis[0]) +\n",
    "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the losses for the entire batch, we can simply input the entire vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis[0]) +\n",
    "  (1 - y_train) * torch.log(1 - hypothesis[0]))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just `.mean()` to take the mean of these individual lossess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: \n",
      " tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print('cost: \\n', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 한 줄이면 간단하게 같은 값을 구할 수 있다.\n",
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros((1), requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD([W,b], lr = 1)\n",
    "\n",
    "# Epochs\n",
    "nb_epochs = 1000\n",
    "\n",
    "# Training\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # Cost\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W)+b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # H(X)\n",
    "    optimizer.zero_grad() # 혹시 이전에 계산한 Gradient 가 있을 수 있으니 0으로 변경\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print log in every 100 epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "After we finish training the model, we want to check how well our model fits the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('/Volumes/SUHYE/1.research/2024_study/PyTorch/data-03-diabetes.csv'\n",
    "                , delimiter=',', dtype= np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "# print('x_data: \\n', x_data) \n",
    "# print('y_data: \\n', y_data)\n",
    "# print('x_train: \\n', x_train)\n",
    "# print('y_train: \\n', y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693147\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539493\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507066\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488209\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693147\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539493\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507066\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488209\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4103],\n",
      "        [0.9242],\n",
      "        [0.2300],\n",
      "        [0.9411],\n",
      "        [0.1772]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5].float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare it with the correcdt labels `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: \n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "\n",
      "y_train: \n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print('prediction: \\n',prediction[:5].float())\n",
    "print('\\ny_train: \\n', y_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "\n",
      "ACC: \n",
      " tensor(0.7668)\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5].float())\n",
    "\n",
    "accuracy = torch.mean(correct_prediction.float())\n",
    "print('\\nACC: \\n', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Implementation with class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Implemenation with Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.474036, Accuracy 76.94%\n",
      "Epoch   10/100 Cost: 0.474033, Accuracy 76.94%\n",
      "Epoch   20/100 Cost: 0.474031, Accuracy 76.94%\n",
      "Epoch   30/100 Cost: 0.474028, Accuracy 76.94%\n",
      "Epoch   40/100 Cost: 0.474026, Accuracy 76.94%\n",
      "Epoch   50/100 Cost: 0.474023, Accuracy 76.94%\n",
      "Epoch   60/100 Cost: 0.474021, Accuracy 76.94%\n",
      "Epoch   70/100 Cost: 0.474018, Accuracy 76.94%\n",
      "Epoch   80/100 Cost: 0.474016, Accuracy 76.94%\n",
      "Epoch   90/100 Cost: 0.474014, Accuracy 76.94%\n",
      "Epoch  100/100 Cost: 0.474011, Accuracy 76.94%\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train) # x가 1일 확률 P(x=1)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print log in every 10 times\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}, Accuracy {:2.2f}%'.format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy*100\n",
    "        ))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latex 기호 모음\n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
